{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T05:19:58.563067400Z",
     "start_time": "2025-06-03T05:19:58.174828500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T05:20:40.306679600Z",
     "start_time": "2025-06-03T05:20:40.185196800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:16:38.377332400Z",
     "start_time": "2025-06-03T06:16:34.265146600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# text2gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:51.239920300Z",
     "start_time": "2025-06-03T06:21:51.228494300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextGlossDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.text_data = [\"hello world\", \"good morning\"]\n",
    "        self.gloss_data = [[\"HELLO\", \"WORLD\"], [\"GOOD\", \"MORNING\"]]\n",
    "        self.text_vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"hello\":3, \"world\":4, \"good\":5, \"morning\":6}\n",
    "        self.gloss_vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"HELLO\":3, \"WORLD\":4, \"GOOD\":5, \"MORNING\":6}\n",
    "        self.inv_gloss = {v:k for k, v in self.gloss_vocab.items()}\n",
    "\n",
    "    def __len__(self): return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.text_vocab[w] for w in self.text_data[idx].split()]\n",
    "        gloss = [self.gloss_vocab[g] for g in self.gloss_data[idx]]\n",
    "        return torch.tensor(text), torch.tensor(gloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:52.168969500Z",
     "start_time": "2025-06-03T06:21:52.137493900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Text2GlossTransformer(nn.Module):\n",
    "    def __init__(self, text_vocab_size, gloss_vocab_size):\n",
    "        super().__init__()\n",
    "        self.text_embed = nn.Embedding(text_vocab_size, 256)\n",
    "        self.gloss_embed = nn.Embedding(gloss_vocab_size, 256)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=256, nhead=8, num_encoder_layers=3, num_decoder_layers=3\n",
    "        ).to(device)\n",
    "        self.fc = nn.Linear(256, gloss_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.text_embed(src).permute(1,0,2) # S, B, E\n",
    "        tgt = self.gloss_embed(tgt).permute(1,0,2)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(device)\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        return self.fc(output).permute(1,0,2) # B, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:53.976701300Z",
     "start_time": "2025-06-03T06:21:53.965553Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Gloss2Pose(nn.Module):\n",
    "    def __init__(self, gloss_vocab_size, pose_dim=51): # 17 key points * 3 (x, y, conf)\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(gloss_vocab_size, 128)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Conv1d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, pose_dim, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, gloss_seq):\n",
    "        x = self.embed(gloss_seq).permute(0,2,1) # B, C, S\n",
    "        return self.conv(x).permute(0,2,1) # B, S, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:54.648783600Z",
     "start_time": "2025-06-03T06:21:54.624890Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def render_pose(pose, connections, frame_size=(512, 512)):\n",
    "    frame = np.zeros((*frame_size, 3), dtype=np.uint8)\n",
    "    keypoints = pose.reshape(-1,3)\n",
    "    keypoints[:, :2] = keypoints[:, :2] * frame_size[0]\n",
    "\n",
    "    # draw connections\n",
    "    for i, j in connections:\n",
    "        if keypoints[i, 2] > 0.2 and keypoints[j, 2] > 0.2: # conf thresh\n",
    "            cv2.line(\n",
    "                frame,\n",
    "                (int(keypoints[i,0]), int(keypoints[i, 1])),\n",
    "                (int(keypoints[j, 0]), int(keypoints[j, 1])),\n",
    "                (255, 166, 2), # orange\n",
    "                2\n",
    "            )\n",
    "\n",
    "\n",
    "    # draw points\n",
    "    for i, (x, y, conf) in enumerate(keypoints):\n",
    "        if conf > 0.2:\n",
    "            cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 255), -1) # yellow\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:58.588530Z",
     "start_time": "2025-06-03T06:21:55.297622900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "  6%|▌         | 3/50 [00:00<00:03, 12.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=2.1472\n",
      "Epoch 2: Loss=1.3307\n",
      "Epoch 3: Loss=0.9652\n",
      "Epoch 4: Loss=0.6635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:00<00:02, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.7357\n",
      "Epoch 6: Loss=0.6038\n",
      "Epoch 7: Loss=0.5030\n",
      "Epoch 8: Loss=0.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:00<00:02, 15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=0.3299\n",
      "Epoch 10: Loss=0.3349\n",
      "Epoch 11: Loss=0.2360\n",
      "Epoch 12: Loss=0.2328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:01<00:02, 16.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=0.1673\n",
      "Epoch 14: Loss=0.0940\n",
      "Epoch 15: Loss=0.1292\n",
      "Epoch 16: Loss=0.0900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:01<00:01, 16.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss=0.1401\n",
      "Epoch 18: Loss=0.0979\n",
      "Epoch 19: Loss=0.0631\n",
      "Epoch 20: Loss=0.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [00:01<00:01, 16.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Loss=0.0345\n",
      "Epoch 22: Loss=0.0336\n",
      "Epoch 23: Loss=0.0308\n",
      "Epoch 24: Loss=0.0255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [00:01<00:01, 16.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Loss=0.0189\n",
      "Epoch 26: Loss=0.0202\n",
      "Epoch 27: Loss=0.0169\n",
      "Epoch 28: Loss=0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:02<00:01, 15.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Loss=0.0128\n",
      "Epoch 30: Loss=0.0183\n",
      "Epoch 31: Loss=0.0171\n",
      "Epoch 32: Loss=0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [00:02<00:00, 15.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Loss=0.0153\n",
      "Epoch 34: Loss=0.0153\n",
      "Epoch 35: Loss=0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [00:02<00:00, 14.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Loss=0.0110\n",
      "Epoch 37: Loss=0.0101\n",
      "Epoch 38: Loss=0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:02<00:00, 13.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Loss=0.0078\n",
      "Epoch 40: Loss=0.0091\n",
      "Epoch 41: Loss=0.0071\n",
      "Epoch 42: Loss=0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [00:03<00:00, 14.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Loss=0.0071\n",
      "Epoch 44: Loss=0.0065\n",
      "Epoch 45: Loss=0.0055\n",
      "Epoch 46: Loss=0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 15.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Loss=0.0059\n",
      "Epoch 48: Loss=0.0075\n",
      "Epoch 49: Loss=0.0060\n",
      "Epoch 50: Loss=0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_index = train_index + 1\n",
    "\n",
    "dataset = TextGlossDataset()\n",
    "\n",
    "t2g_model = Text2GlossTransformer(\n",
    "    len(dataset.text_vocab),\n",
    "    len(dataset.gloss_vocab)\n",
    ").to(device)\n",
    "\n",
    "g2p_model = Gloss2Pose(len(dataset.gloss_vocab)).to(device)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(t2g_model.parameters()) + list(g2p_model.parameters()),\n",
    "    lr = 1e-4\n",
    ")\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in tqdm.tqdm(range(50)):\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        decoder_input = torch.cat([\n",
    "            torch.ones_like(tgt[:, :1]) * dataset.gloss_vocab[\"<sos>\"],\n",
    "            tgt[:, :-1]\n",
    "        ], dim=1)\n",
    "\n",
    "        gloss_logits = t2g_model(src, decoder_input)\n",
    "        # gloss_loss = nn.CrossEntropyLoss()(\n",
    "        #     gloss_logits.view(-1, gloss_logits.size(-1)),\n",
    "        #     tgt.view(-1)\n",
    "        # )\n",
    "\n",
    "        gloss_logits = gloss_logits.contiguous()\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(\n",
    "            gloss_logits.view(-1, gloss_logits.size(-1)),\n",
    "            tgt.contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}: Loss={loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:22:02.943543300Z",
     "start_time": "2025-06-03T06:22:02.919435200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Loss' : losses\n",
    "})\n",
    "\n",
    "df.to_csv(f'train{train_index}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:22:04.073494Z",
     "start_time": "2025-06-03T06:22:04.054671100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_to_sign(text):\n",
    "    tokens = [dataset.text_vocab.get(w, 0) for w in text.split()]\n",
    "    src_tokens = torch.tensor([tokens]).to(device)\n",
    "\n",
    "    gloss_seq = [dataset.gloss_vocab[\"<sos>\"]]\n",
    "    for i in tqdm.tqdm(range(20)):\n",
    "        gloss_decoder_input = torch.tensor([gloss_seq]).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = t2g_model(src_tokens, gloss_decoder_input)\n",
    "        next_id = logits[0, -1].argmax().item()\n",
    "        if next_id == dataset.gloss_vocab[\"<eos>\"]:\n",
    "            break\n",
    "        gloss_seq.append(next_id)\n",
    "\n",
    "    glosses = [dataset.inv_gloss[idx] for idx in gloss_seq[1:]]\n",
    "    print(f\"Glosses: {' '.join(glosses)}\")\n",
    "\n",
    "    gloss_tensor = torch.tensor([gloss_seq[1:]]).to(device)  # Exclude <sos>\n",
    "    with torch.no_grad():\n",
    "        poses = g2p_model(gloss_tensor).cpu().numpy()[0]  # [S, D]\n",
    "\n",
    "    connections = [\n",
    "        (0,1), (0,2), (1,3), (2,4),        # Head\n",
    "        (5,6), (5,7), (7,9), (6,8), (8,10), # Arms\n",
    "        (11,12), (11,13), (13,15), (12,14), (14,16) # Legs\n",
    "    ]\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(f'sign_output{train_index}.mp4', fourcc, 5.0, (512, 512))\n",
    "\n",
    "    for pose in poses:\n",
    "        pose_norm = (pose - np.min(pose)) / (np.max(pose) - np.min(pose) + 1e-8)\n",
    "        frame = render_pose(pose_norm, connections)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    return glosses, f'sign_output{train_index}.mp4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:22:06.505088900Z",
     "start_time": "2025-06-03T06:22:05.102502600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 70.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glosses: HELLO WORLD HELLO WORLD WORLD WORLD WORLD WORLD WORLD WORLD WORLD WORLD WORLD WORLD WORLD WORLD HELLO WORLD WORLD WORLD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['HELLO',\n",
       "  'WORLD',\n",
       "  'HELLO',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'HELLO',\n",
       "  'WORLD',\n",
       "  'WORLD',\n",
       "  'WORLD'],\n",
       " 'sign_output7.mp4')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sign(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:15.809143300Z",
     "start_time": "2025-06-03T06:21:15.797312200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

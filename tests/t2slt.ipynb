{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T05:19:58.563067400Z",
     "start_time": "2025-06-03T05:19:58.174828500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_index = date.today().toordinal()\n",
    "\n",
    "DATA_PATH = '../../datasets/train-00000-of-00001.parquet'\n",
    "OUTPUT_PATH = '../../datasets/Processed/Gloss_feat/processed_data.pt'\n",
    "\n",
    "G2P_PATH = '../../datasets/archive/'\n",
    "G2P_PATH_JSON = '../../datasets/archive/WLASL_v0.3.json'\n",
    "\n",
    "MAX_SEQ_LEN = 50\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(df):\n",
    "    \n",
    "    text_counter = Counter()\n",
    "    gloss_counter = Counter()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        full_text = row[\"text\"]\n",
    "        \n",
    "        gloss = full_text.split(\"[INST]\")[1].split(\"[/INST]\")[0].strip()\n",
    "        gloss_tokens = gloss.split()\n",
    "        gloss_counter.update(gloss_tokens)\n",
    "        \n",
    "        text = full_text.split(\"[/INST]\")[1].replace(\"</s>\", \"\").strip().lower()\n",
    "        text_tokens = text.split()\n",
    "        text_counter.update(text_tokens)\n",
    "        \n",
    "        base_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "        \n",
    "        text_vocab = {tok: idx for idx, tok in enumerate(base_tokens)}\n",
    "        for tok, _ in text_counter.most_common():\n",
    "            if tok not in text_vocab:\n",
    "                text_vocab[tok] = len(text_vocab)\n",
    "                \n",
    "        gloss_vocab = {tok: idx for idx, tok in enumerate(base_tokens)}\n",
    "        for tok, _ in gloss_counter.most_common():\n",
    "            if tok not in gloss_vocab:\n",
    "                gloss_vocab[tok] = len(gloss_vocab)\n",
    "                \n",
    "        return text_vocab, gloss_vocab\n",
    "\n",
    "def tokenize_and_pad(sequence, vocab, max_len, is_gloss):\n",
    "    if is_gloss:\n",
    "        tokens = sequence.split(\"[INST]\")[1].split(\"[/INST]\")[0].strip().split()\n",
    "    else:\n",
    "        tokens = sequence.split(\"[/INST]\")[1].replace(\"</s>\", \"\").strip().lower().split()\n",
    "    \n",
    "    idxs = [vocab.get(w, vocab[\"<unk>\"]) for w in tokens]\n",
    "    idxs = [vocab[\"<sos>\"]] + idxs + [vocab[\"<eos>\"]]\n",
    "    \n",
    "    if len(idxs) < max_len:\n",
    "        idxs = idxs + [vocab[\"<pad>\"]] * (max_len - len(idxs))\n",
    "    else:\n",
    "        idxs = idxs[: max_len - 1] + [vocab[\"<eos>\"]]\n",
    "\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Built text_vocab (size: 17) and gloss_vocab (size: 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 5000/5000 [00:02<00:00, 2301.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Saved processed data to ../../datasets/Processed/Gloss_feat/processed_data.pt\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "text_vocab, gloss_vocab = build_vocab(df)\n",
    "print(f\"> Built text_vocab (size: {len(text_vocab)}) and gloss_vocab (size: {len(gloss_vocab)})\")\n",
    "\n",
    "all_text_tensors  = []\n",
    "all_gloss_tensors = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Tokenizing\"):\n",
    "    raw = row[\"text\"]\n",
    "    gloss_tensor = tokenize_and_pad(raw, gloss_vocab, max_len=MAX_SEQ_LEN, is_gloss=True)\n",
    "    text_tensor  = tokenize_and_pad(raw, text_vocab,  max_len=MAX_SEQ_LEN, is_gloss=False)\n",
    "\n",
    "    all_gloss_tensors.append(gloss_tensor.to(device))\n",
    "    all_text_tensors.append(text_tensor.to(device))\n",
    "\n",
    "gloss_matrix = torch.stack(all_gloss_tensors)   # (N, MAX_SEQ_LEN)\n",
    "text_matrix  = torch.stack(all_text_tensors)    # ( N,MAX_SEQ_LEN)\n",
    "\n",
    "save_dict = {\n",
    "    \"text_vocab\": text_vocab,\n",
    "    \"gloss_vocab\": gloss_vocab,\n",
    "    \"inv_gloss\": {v: k for k, v in gloss_vocab.items()},\n",
    "    \"text_matrix\": text_matrix,   # torch.LongTensor\n",
    "    \"gloss_matrix\": gloss_matrix, # torch.LongTensor\n",
    "}\n",
    "torch.save(save_dict, OUTPUT_PATH)\n",
    "print(f\"> Saved processed data to {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### txtGlossDataset CLass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:51.239920300Z",
     "start_time": "2025-06-03T06:21:51.228494300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextGlossDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.text_data = [\"hello world\", \"good morning\"]\n",
    "        self.gloss_data = [[\"HELLO\", \"WORLD\"], [\"GOOD\", \"MORNING\"]]\n",
    "        self.text_vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"hello\":3, \"world\":4, \"good\":5, \"morning\":6}\n",
    "        self.gloss_vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"HELLO\":3, \"WORLD\":4, \"GOOD\":5, \"MORNING\":6}\n",
    "        self.inv_gloss = {v:k for k, v in self.gloss_vocab.items()}\n",
    "\n",
    "    def __len__(self): return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [self.text_vocab[w] for w in self.text_data[idx].split()]\n",
    "        gloss = [self.gloss_vocab[g] for g in self.gloss_data[idx]]\n",
    "        return torch.tensor(text), torch.tensor(gloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGlossDataset2(Dataset):\n",
    "    def __init__(self, parquet_path, max_seq_length=50):\n",
    "        \n",
    "        self.df = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        self.text_vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
    "        self.gloss_vocab = {\"<pad>\":0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\":3}\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        self._build_vocabs()\n",
    "        \n",
    "        self.inv_gloss = {v: k for k, v in self.gloss_vocab.items()}\n",
    "        \n",
    "    def _build_vocabs(self):\n",
    "        \n",
    "        text_words = []\n",
    "        gloss_words = []\n",
    "        \n",
    "        for _, row in self.df.iterrows():\n",
    "            \n",
    "            gloss = row['text'].split('[INST]')[1].split('[/INST]')[0].strip()\n",
    "            gloss_words.extend(gloss.split())\n",
    "            \n",
    "            text = row['text'].split('[/INST]')[1].replace('</s>', '').strip()\n",
    "            text_words.extend(text.lower().split())\n",
    "            \n",
    "            # build text vocab\n",
    "            text_counter = Counter(text_words)\n",
    "            for word, _ in text_counter.most_common():\n",
    "                if word not in self.text_vocab:\n",
    "                    self.text_vocab[word] = len(self.text_vocab)\n",
    "            \n",
    "            #build gloss vocab\n",
    "            gloss_counter = Counter(gloss_words)\n",
    "            for gloss, _ in gloss_counter.most_common():\n",
    "                if gloss not in self.gloss_vocab:\n",
    "                    self.gloss_vocab[gloss] = len(self.gloss_vocab)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _process_sequence(self, sequence, vocab, is_gloss=False):\n",
    "        \n",
    "        if is_gloss:\n",
    "            seq = sequence.split('[INST]')[1].split('[/INST]')[0].strip().split()\n",
    "        else:\n",
    "            seq = sequence.split('[/INST]')[1].replace('</s>', '').strip().lower().split()\n",
    "        \n",
    "        indices = [vocab.get(word, vocab[\"<unk>\"]) for word in seq]\n",
    "        indices = [vocab[\"<sos>\"]] + indices + [vocab[\"<eos>\"]]\n",
    "        \n",
    "        if len(indices) < self.max_seq_length:\n",
    "            indices = indices + [vocab[\"<pad>\"]] * (self.max_seq_length - len(indices))\n",
    "        else:\n",
    "            indices = indices[:self.max_seq_length-1] + [vocab[\"<eos>\"]]\n",
    "        \n",
    "        return torch.tensor(indices).to(device)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]['text']\n",
    "        \n",
    "        gloss = self._process_sequence(row, self.gloss_vocab, is_gloss=True)\n",
    "        text = self._process_sequence(row, self.text_vocab, is_gloss=False)\n",
    "        \n",
    "        return text, gloss\n",
    "    \n",
    "    def decode_gloss(self, indices):\n",
    "        return ' '.join([self.inv_gloss.get(idx, '<unk>') for idx in indices if idx not in {0, 1, 2}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGlossDataset3(Dataset): # takes from .pth\n",
    "    def __init__(self, processed_path):\n",
    "        \n",
    "        data = torch.load(processed_path, map_location=torch.device(\"cpu\"))\n",
    "        self.text_vocab  = data[\"text_vocab\"]\n",
    "        self.gloss_vocab = data[\"gloss_vocab\"]\n",
    "        self.inv_gloss   = data[\"inv_gloss\"]\n",
    "\n",
    "        # Pre‐tokenized (N, max_seq_len)\n",
    "        self.text_matrix  = data[\"text_matrix\"]\n",
    "        self.gloss_matrix = data[\"gloss_matrix\"]\n",
    "\n",
    "        assert self.text_matrix.size(0) == self.gloss_matrix.size(0), \"Mismatch in example count\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.text_matrix.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_indices  = self.text_matrix[idx]\n",
    "        gloss_indices = self.gloss_matrix[idx]\n",
    "        return text_indices, gloss_indices\n",
    "\n",
    "    def decode_gloss(self, indices):\n",
    "        return \" \".join(\n",
    "            [self.inv_gloss.get(int(idx), \"<unk>\") for idx in indices if idx not in {0, 1, 2}]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G2P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WLASLGlossPoseDataset(Dataset):\n",
    "    def __init__(self, json_path, video_dir, gloss_vocab=None, max_samples=100, cache_dir=\"pose_cache\", force_reprocess=False):\n",
    "        \"\"\"\n",
    "        WLASL Gloss to Pose Dataset\n",
    "        Args:\n",
    "            json_path: Path to WLASL_v0.3.json\n",
    "            video_dir: Directory containing video files\n",
    "            gloss_vocab: Existing gloss vocabulary (or create new)\n",
    "            max_samples: Maximum samples to process\n",
    "            cache_dir: Directory to store extracted poses\n",
    "            force_reprocess: Reprocess even if cached exists\n",
    "        \"\"\"\n",
    "        self.video_dir = video_dir\n",
    "        self.cache_dir = cache_dir\n",
    "        self.force_reprocess = force_reprocess\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if gloss_vocab is None:\n",
    "            self.gloss_vocab = {}\n",
    "            for i, entry in enumerate(data):\n",
    "                self.gloss_vocab[entry['gloss']] = i\n",
    "        else:\n",
    "            self.gloss_vocab = gloss_vocab\n",
    "            \n",
    "        self.samples = []\n",
    "        for entry in data[:max_samples]:\n",
    "            gloss = entry['gloss']\n",
    "            for instance in entry['instances']:\n",
    "                video_id = instance['video_id']\n",
    "                video_path = os.path.join(video_dir, f\"{video_id}.mp4\")\n",
    "                if os.path.exists(video_path):\n",
    "                    self.samples.append((gloss, video_path, video_id))\n",
    "        \n",
    "        # Initialize MediaPipe Pose\n",
    "        self.mp_pose = mp.solutions.pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2,\n",
    "            enable_segmentation=False,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gloss, video_path, video_id = self.samples[idx]\n",
    "        cache_path = os.path.join(self.cache_dir, f\"{video_id}.pt\")\n",
    "        \n",
    "        # Load from cache or process video\n",
    "        if os.path.exists(cache_path) and not self.force_reprocess:\n",
    "            pose_seq = torch.load(cache_path)\n",
    "        else:\n",
    "            pose_seq = self.process_video(video_path)\n",
    "            torch.save(pose_seq, cache_path)\n",
    "        \n",
    "        return {\n",
    "            'gloss': self.gloss_vocab[gloss],\n",
    "            'pose': pose_seq,\n",
    "            'video_id': video_id\n",
    "        }\n",
    "    \n",
    "    def process_video(self, video_path):\n",
    "        # extract pose sequence from video using\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        pose_sequence = []\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Process frame with MediaPipe\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = self.mp_pose.process(frame_rgb)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                # Extract 33 pose landmarks (x, y, visibility)\n",
    "                landmarks = []\n",
    "                for landmark in results.pose_landmarks.landmark:\n",
    "                    landmarks.extend([landmark.x, landmark.y, landmark.visibility])\n",
    "                pose_sequence.append(landmarks)\n",
    "            else:\n",
    "                # Pad with zeros if no detection\n",
    "                pose_sequence.append([0.0]*99)\n",
    "        \n",
    "        cap.release()\n",
    "        return torch.tensor(pose_sequence, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'WLASL_v0.3.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mWLASLGlossPoseDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWLASL_v0.3.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m  \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m sample \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGloss ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m, in \u001b[0;36mWLASLGlossPoseDataset.__init__\u001b[1;34m(self, json_path, video_dir, gloss_vocab, max_samples, cache_dir, force_reprocess)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_reprocess \u001b[38;5;241m=\u001b[39m force_reprocess\n\u001b[0;32m     16\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(cache_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     19\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gloss_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'WLASL_v0.3.json'"
     ]
    }
   ],
   "source": [
    "dataset = WLASLGlossPoseDataset(\n",
    "    json_path=\"WLASL_v0.3.json\",\n",
    "    video_dir=\"videos\",\n",
    "    max_samples=100  \n",
    ")\n",
    "\n",
    "sample = dataset[0]\n",
    "print(f\"Gloss ID: {sample['gloss']}\")\n",
    "print(f\"Pose sequence shape: {sample['pose'].shape}\")  # (num_frames, 33*3=99)\n",
    "print(f\"Video ID: {sample['video_id']}\")\n",
    "\n",
    "# DataLoader with padding\n",
    "def collate_fn(batch):\n",
    "    glosses = torch.tensor([item['gloss'] for item in batch])\n",
    "    poses = [item['pose'] for item in batch]\n",
    "    video_ids = [item['video_id'] for item in batch]\n",
    "    \n",
    "    # Pad pose sequences to same length\n",
    "    poses_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        poses, batch_first=True, padding_value=0.0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'gloss': glosses,\n",
    "        'pose': poses_padded,\n",
    "        'pose_lengths': torch.tensor([len(p) for p in poses]),\n",
    "        'video_id': video_ids\n",
    "    }\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for batch in loader:\n",
    "    print(\"\\nBatch:\")\n",
    "    print(f\"Glosses: {batch['gloss']}\")\n",
    "    print(f\"Poses shape: {batch['pose'].shape}\")  # (batch, max_frames, 99)\n",
    "    print(f\"Pose lengths: {batch['pose_lengths']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-Gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:52.168969500Z",
     "start_time": "2025-06-03T06:21:52.137493900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Text2GlossTransformer(nn.Module):\n",
    "    def __init__(self, text_vocab_size, gloss_vocab_size):\n",
    "        super().__init__()\n",
    "        self.text_embed = nn.Embedding(text_vocab_size, 256)\n",
    "        self.gloss_embed = nn.Embedding(gloss_vocab_size, 256)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=256, nhead=8, num_encoder_layers=3, num_decoder_layers=3\n",
    "        ).to(device)\n",
    "        self.fc = nn.Linear(256, gloss_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.text_embed(src).permute(1,0,2) # S, B, E\n",
    "        tgt = self.gloss_embed(tgt).permute(1,0,2)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(device)\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        return self.fc(output).permute(1,0,2) # B, S, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gloss-to-Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:53.976701300Z",
     "start_time": "2025-06-03T06:21:53.965553Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Gloss2Pose(nn.Module):\n",
    "    def __init__(self, gloss_vocab_size, pose_dim=51): # 17 key points * 3 (x, y, conf)\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(gloss_vocab_size, 128)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Conv1d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, pose_dim, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, gloss_seq):\n",
    "        x = self.embed(gloss_seq).permute(0,2,1) # B, C, S\n",
    "        return self.conv(x).permute(0,2,1) # B, S, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PoseVQVAE(nn.Module):\n",
    "    def __init__(self, pose_dim=51, num_tokens=1024, embedding_dim=64, hidden_dim=256):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(pose_dim, hidden_dim, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, embedding_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.codebook = nn.Embedding(num_tokens, embedding_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(embedding_dim, hidden_dim, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, pose_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, poses):\n",
    "        # (B, T, D)\n",
    "        x = poses.permute(0,2,1) # (B, D, T)\n",
    "        z = self.encoder(x) # (B, E, T)\n",
    "        z = z.permute(0,2,1) # (B,T,E)\n",
    "        \n",
    "        #quanntization\n",
    "        distance = torch.cdist(z, self.codebook.weight) #(B, T, K)\n",
    "        tokens = distance.argmin(-1) # (B, T)\n",
    "        quantized = self.codebook(tokens)\n",
    "        \n",
    "        # decode\n",
    "        quantized = quantized.permute(0,2,1) # (B,E, T)\n",
    "        recon = self.decoder(quantized).permute(0,2,1) # (B,T,D)\n",
    "        \n",
    "        #losses\n",
    "        recon_loss = F.mse_loss(recon, poses)\n",
    "        commit_loss = F.mse_loss(z.detach(), quantized)\n",
    "        return recon, recon_loss + 0.25 * commit_loss, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2PWithVQ(nn.Module):\n",
    "    def __init__(self, gloss_vocab_size,pose_dim = 51, num_tokens = 1024, k=5):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.vqvae = PoseVQVAE(pose_dim, num_tokens)\n",
    "        \n",
    "        self.embed = nn.Embedding(gloss_vocab_size, 128)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, num_tokens, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, gloss_seq):\n",
    "        \n",
    "        x = self.embed(gloss_seq)\n",
    "        \n",
    "        #unsampling\n",
    "        x = x.repeat_interleave(self.k, dim=1) # (B, S*k, 128)\n",
    "        x = x.permute(0,2,1) # (V, T, num_tokens)\n",
    "        \n",
    "        logits = self.conv(x).permute(0,2,1) # (B, T, num_tokens)\n",
    "        return logits\n",
    "    \n",
    "    def decode(self, logits):\n",
    "        # logits -> Pose seqs\n",
    "        pred_tokens = logits.argmax(-1)\n",
    "        return self.vqvae.decoder(\n",
    "            self.vqvae.codebook(pred_tokens).permute(0,2,1)\n",
    "        ).permute(0,2,1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:54.648783600Z",
     "start_time": "2025-06-03T06:21:54.624890Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def render_pose(pose, connections, frame_size=(512, 512)):\n",
    "    frame = np.zeros((*frame_size, 3), dtype=np.uint8)\n",
    "    keypoints = pose.reshape(-1,3)\n",
    "    keypoints[:, :2] = keypoints[:, :2] * frame_size[0]\n",
    "\n",
    "    # draw connections\n",
    "    for i, j in connections:\n",
    "        if keypoints[i, 2] > 0.2 and keypoints[j, 2] > 0.2: # conf thresh\n",
    "            cv2.line(\n",
    "                frame,\n",
    "                (int(keypoints[i,0]), int(keypoints[i, 1])),\n",
    "                (int(keypoints[j, 0]), int(keypoints[j, 1])),\n",
    "                (255, 166, 2), # orange\n",
    "                2\n",
    "            )\n",
    "\n",
    "\n",
    "    # draw points\n",
    "    for i, (x, y, conf) in enumerate(keypoints):\n",
    "        if conf > 0.2:\n",
    "            cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 255), -1) # yellow\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:58.588530Z",
     "start_time": "2025-06-03T06:21:55.297622900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "  3%|▎         | 1/30 [02:30<1:12:40, 150.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [05:10<1:12:52, 156.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.1692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [07:39<1:08:44, 152.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.0529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [10:15<1:06:52, 154.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.0378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [12:48<1:04:01, 153.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.0435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [15:09<59:46, 149.43s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=0.0527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [17:54<59:15, 154.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=0.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [24:22<1:23:53, 228.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [27:18<1:14:18, 212.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=0.0417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [30:13<1:06:53, 200.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=0.0669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [33:05<1:00:49, 192.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=0.0659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [35:56<55:41, 185.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=0.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [38:48<51:22, 181.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=0.0560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [41:38<47:29, 178.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=0.0284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [44:30<44:02, 176.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss=0.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [47:20<40:40, 174.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Loss=0.0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [50:20<38:10, 176.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss=0.0342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [52:54<33:52, 169.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss=0.1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [55:52<31:32, 172.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss=0.0576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [58:48<28:52, 173.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss=0.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [1:01:43<26:03, 173.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Loss=0.1748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [1:04:34<23:03, 173.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Loss=0.0313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [1:07:24<20:04, 172.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Loss=0.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [1:10:14<17:08, 171.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Loss=0.0345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [1:13:11<14:24, 172.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Loss=0.0995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [1:16:13<11:42, 175.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Loss=0.0513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [1:18:49<08:29, 169.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Loss=0.1861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [1:21:40<05:40, 170.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Loss=0.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [1:24:33<02:51, 171.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Loss=0.0959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:27:19<00:00, 174.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Loss=0.0528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_index = train_index + 1\n",
    "\n",
    "# dataset = TextGlossDataset2('../../datasets/train-00000-of-00001.parquet')\n",
    "dataset = TextGlossDataset3(OUTPUT_PATH)\n",
    "\n",
    "t2g_model = Text2GlossTransformer(\n",
    "    len(dataset.text_vocab),\n",
    "    len(dataset.gloss_vocab)\n",
    ").to(device)\n",
    "\n",
    "g2p_model = Gloss2Pose(len(dataset.gloss_vocab)).to(device)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(t2g_model.parameters()) + list(g2p_model.parameters()),\n",
    "    lr = 1e-4\n",
    ")\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in tqdm(range(30)):\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        decoder_input = torch.cat([\n",
    "            torch.ones_like(tgt[:, :1]) * dataset.gloss_vocab[\"<sos>\"],\n",
    "            tgt[:, :-1]\n",
    "        ], dim=1)\n",
    "\n",
    "        gloss_logits = t2g_model(src, decoder_input)\n",
    "        # gloss_loss = nn.CrossEntropyLoss()(\n",
    "        #     gloss_logits.view(-1, gloss_logits.size(-1)),\n",
    "        #     tgt.view(-1)\n",
    "        # )\n",
    "\n",
    "        gloss_logits = gloss_logits.contiguous()\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(\n",
    "            gloss_logits.view(-1, gloss_logits.size(-1)),\n",
    "            tgt.contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}: Loss={loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2g_losses = []\n",
    "g2p_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(t2g_model.state_dict(), 't2g_model_weights.pth')\n",
    "torch.save(g2p_model.state_dict(), 'g2p_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(t2g_model, 't2g_model.pth')\n",
    "torch.save(g2p_model, 'g2p_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_vqvae(model, dataset, epochs=10):\n",
    "    loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            _, _, pose = batch \n",
    "            pose = pose.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, loss, _ = model(pose)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"VQ-VAE Pre-train Epoch {epoch+1}: Loss={loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2():\n",
    "    dataset = TextGlossDataset3(OUTPUT_PATH)\n",
    "    \n",
    "    t2g_model = Text2GlossTransformer(\n",
    "        len(dataset.text_vocab),\n",
    "        len(dataset.gloss_vocab)\n",
    "    ).to(device)\n",
    "    \n",
    "    g2p_model = G2PWithVQ(len(dataset.gloss_vocab)).to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    t2g_optim = torch.optim.Adam(t2g_model.parameters(), lr=1e-4)\n",
    "    g2p_optim = torch.optim.Adam(g2p_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # pre-train the VQ-VAE component\n",
    "    print(\"Pre-training VQ-VAE...\")\n",
    "    pretrain_vqvae(g2p_model.vqvae, dataset)  \n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        for batch in loader:\n",
    "            # batch: (text, gloss, pose)\n",
    "            text, gloss, pose = batch\n",
    "            text, gloss, pose = text.to(device), gloss.to(device), pose.to(device)\n",
    "            \n",
    "            t2g_optim.zero_grad()\n",
    "            decoder_input = torch.cat([\n",
    "                torch.ones_like(gloss[:, :1]) * dataset.gloss_vocab[\"<sos>\"],\n",
    "                gloss[:, :-1]\n",
    "            ], dim=1)\n",
    "            \n",
    "            gloss_logits = t2g_model(text, decoder_input)\n",
    "            t2g_loss = F.cross_entropy(\n",
    "                gloss_logits.view(-1, gloss_logits.size(-1)),\n",
    "                gloss.contiguous().view(-1)\n",
    "            )\n",
    "            t2g_loss.backward()\n",
    "            t2g_optim.step()\n",
    "            \n",
    "            # Gloss to Pose\n",
    "            g2p_optim.zero_grad()\n",
    "            pred_tokens = g2p_model(gloss)\n",
    "            \n",
    "            # target tokens from VQ-VAE\n",
    "            with torch.no_grad():\n",
    "                _, _, target_tokens = g2p_model.vqvae(pose)\n",
    "            \n",
    "            g2p_loss = F.cross_entropy(\n",
    "                pred_tokens.view(-1, pred_tokens.size(-1)),\n",
    "                target_tokens.view(-1)\n",
    "            )\n",
    "            g2p_loss.backward()\n",
    "            g2p_optim.step()\n",
    "            \n",
    "            t2g_losses.append(t2g_loss.item())\n",
    "            g2p_losses.append(g2p_loss.item())\n",
    "        print(f\"Epoch {epoch+1}: T2G Loss={t2g_loss.item():.4f}, G2P Loss={g2p_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training VQ-VAE...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m, in \u001b[0;36mtrain2\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# pre-train the VQ-VAE component\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-training VQ-VAE...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mpretrain_vqvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg2p_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvqvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     20\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m, in \u001b[0;36mpretrain_vqvae\u001b[1;34m(model, dataset, epochs)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m----> 7\u001b[0m         _, _, pose \u001b[38;5;241m=\u001b[39m batch \n\u001b[0;32m      8\u001b[0m         pose \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "train2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:22:02.943543300Z",
     "start_time": "2025-06-03T06:22:02.919435200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Loss' : losses\n",
    "})\n",
    "\n",
    "df.to_csv(f'train{train_index}.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-Sign testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:22:04.073494Z",
     "start_time": "2025-06-03T06:22:04.054671100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_to_sign(text):\n",
    "    tokens = [dataset.text_vocab.get(w, 0) for w in text.split()]\n",
    "    src_tokens = torch.tensor([tokens]).to(device)\n",
    "\n",
    "    gloss_seq = [dataset.gloss_vocab[\"<sos>\"]]\n",
    "    for i in tqdm(range(20)):\n",
    "        gloss_decoder_input = torch.tensor([gloss_seq]).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = t2g_model(src_tokens, gloss_decoder_input)\n",
    "        next_id = logits[0, -1].argmax().item()\n",
    "        if next_id == dataset.gloss_vocab[\"<eos>\"]:\n",
    "            break\n",
    "        gloss_seq.append(next_id)\n",
    "\n",
    "    glosses = [dataset.inv_gloss[idx] for idx in gloss_seq[1:]]\n",
    "    print(f\"Glosses: {' '.join(glosses)}\")\n",
    "\n",
    "    gloss_tensor = torch.tensor([gloss_seq[1:]]).to(device)  # Exclude <sos>\n",
    "    with torch.no_grad():\n",
    "        poses = g2p_model(gloss_tensor).cpu().numpy()[0]  # [S, D]\n",
    "\n",
    "    connections = [\n",
    "        (0,1), (0,2), (1,3), (2,4),        # Head\n",
    "        (5,6), (5,7), (7,9), (6,8), (8,10), # Arms\n",
    "        (11,12), (11,13), (13,15), (12,14), (14,16) # Legs\n",
    "    ]\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(f'sign_output{train_index}.mp4', fourcc, 5.0, (512, 512))\n",
    "\n",
    "    for pose in poses:\n",
    "        pose_norm = (pose - np.min(pose)) / (np.max(pose) - np.min(pose) + 1e-8)\n",
    "        frame = render_pose(pose_norm, connections)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    return glosses, f'sign_output{train_index}.mp4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:22:06.505088900Z",
     "start_time": "2025-06-03T06:22:05.102502600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 37.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glosses: <sos> <sos> <sos> <sos> <sos> <sos> <sos> <sos> <sos> <sos> <sos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['<sos>',\n",
       "  '<sos>',\n",
       "  '<sos>',\n",
       "  '<sos>',\n",
       "  '<sos>',\n",
       "  '<sos>',\n",
       "  '<sos>',\n",
       "  '<sos>',\n",
       "  '<sos>',\n",
       "  '<sos>',\n",
       "  '<sos>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>',\n",
       "  '<unk>'],\n",
       " 'sign_output739415.mp4')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sign(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T06:21:15.809143300Z",
     "start_time": "2025-06-03T06:21:15.797312200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
